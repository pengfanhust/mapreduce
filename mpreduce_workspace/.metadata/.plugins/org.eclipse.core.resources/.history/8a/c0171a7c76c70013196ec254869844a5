/**14/04/04 10:10:47 INFO mapred.JobClient: Job complete: job_local1739076257_0001
14/04/04 10:10:47 INFO mapred.JobClient: Counters: 20
14/04/04 10:10:47 INFO mapred.JobClient:   File Output Format Counters 
14/04/04 10:10:47 INFO mapred.JobClient:     Bytes Written=694579942
14/04/04 10:10:47 INFO mapred.JobClient:   FileSystemCounters
14/04/04 10:10:47 INFO mapred.JobClient:     FILE_BYTES_READ=49053984450
14/04/04 10:10:47 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=38748700127
14/04/04 10:10:47 INFO mapred.JobClient:   File Input Format Counters 
14/04/04 10:10:47 INFO mapred.JobClient:     Bytes Read=1319441569
14/04/04 10:10:47 INFO mapred.JobClient:   Map-Reduce Framework
14/04/04 10:10:47 INFO mapred.JobClient:     Map output materialized bytes=853318690
14/04/04 10:10:47 INFO mapred.JobClient:     Map input records=85331845
14/04/04 10:10:47 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/04/04 10:10:47 INFO mapred.JobClient:     Spilled Records=327427470
14/04/04 10:10:47 INFO mapred.JobClient:     Map output bytes=682654760
14/04/04 10:10:47 INFO mapred.JobClient:     Total committed heap usage (bytes)=6833549312
14/04/04 10:10:47 INFO mapred.JobClient:     CPU time spent (ms)=0
14/04/04 10:10:47 INFO mapred.JobClient:     SPLIT_RAW_BYTES=5760
14/04/04 10:10:47 INFO mapred.JobClient:     Combine input records=0
14/04/04 10:10:47 INFO mapred.JobClient:     Reduce input records=85331845
14/04/04 10:10:47 INFO mapred.JobClient:     Reduce input groups=8768419
14/04/04 10:10:47 INFO mapred.JobClient:     Combine output records=0
14/04/04 10:10:47 INFO mapred.JobClient:     Physical memory (bytes) snapshot=0
14/04/04 10:10:47 INFO mapred.JobClient:     Reduce output records=8768419
14/04/04 10:10:47 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=0
14/04/04 10:10:47 INFO mapred.JobClient:     Map output records=85331845

Fri Apr 04 10:04:21 PDT 2014
Fri Apr 04 10:10:47 PDT 2014
 */

package preprocess;

import java.io.IOException;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

import util.MpRdcConstants;

/**
 * This job generated the adjacent list graph file, without combiner.
 * @author pengfan
 *
 */
public class PreprocessGraph {

	public static class PreprocessGraphMapper extends
			Mapper<Object, Text, IntWritable, IntWritable> {

		private IntWritable followerID = new IntWritable();
		private IntWritable id = new IntWritable();

		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {
			String[] lines = value.toString().split(",");
			followerID.set(Integer.parseInt(lines[0]));
			id.set(Integer.parseInt(lines[1]));

			context.write(followerID, id);
		}
	}

	public static class PreprocessGraphReducer extends
			Reducer<IntWritable, IntWritable, IntWritable, Text> {
		private Text adjacentList = new Text();

		public void reduce(IntWritable key, Iterable<IntWritable> values,
				Context context) throws IOException, InterruptedException {
			StringBuilder ajacentListstr = new StringBuilder();
			for (IntWritable val : values) {
				ajacentListstr.append(val.toString()).append(
						MpRdcConstants.ATTR_SEPRATOR);
			}
			int end = ajacentListstr.length();
			adjacentList.set(ajacentListstr.deleteCharAt(end - 1).toString());
			context.write(key, adjacentList);
		}
	}

	public static void main(String[] args) throws Exception {
		Date start = new Date();
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args)
				.getRemainingArgs();
		if (otherArgs.length != 2) {
			System.err.println("Usage: PreprocessGraph <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf, "PreprocessGraph");
		job.setJarByClass(PreprocessGraph.class);
		job.setMapperClass(PreprocessGraphMapper.class);
		job.setReducerClass(PreprocessGraphReducer.class);
		job.setMapOutputKeyClass(IntWritable.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(Text.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		job.waitForCompletion(true);
		System.out.println(start);
		System.out.println(new Date());
	}
}
